{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ee8da",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Definition of a Random Forest\n",
    "---\n",
    "\n",
    "### Improving on Decision Trees\n",
    "\n",
    "The main drawback of decision trees is their tendency to overfit. We can improve their performance with pruning techniques, but this module will introduce a better model. \n",
    "\n",
    "Decision trees are very susceptible to random idiosyncrasies in the training dataset. They have very high variance, which means if you randomly select the training dataset, you end up with very different models.\n",
    "\n",
    "One advantage of decision trees over models like logistic regression is that they make no assumptions about how the data is structured. In logistic regression, we assume to draw a line to split the data points. Sometimes when the data is not structured like that, decision trees can still capture the essence of the data.\n",
    "\n",
    "This module is about **random forests**. As its name suggests, it is a model built with multiple trees. The goal of random forests is to take advantage of decision trees while mitigating the variance issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e2f73",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "A **bootstrapping** sample are data points randomly selected from our original dataset with replacement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
